{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import progressbar\n",
    "import _pickle as cPickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from SenTree import SenTree\n",
    "from tree_lstm import TreeLSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Var(v):\n",
    "    return Variable(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trees, targets, labels = SenTree.getTrees(file='trees/train_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = list(zip(trees, targets, labels))\n",
    "np.random.shuffle(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = total[:3000]\n",
    "dev_data = total[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TreeLSTM(SenTree.vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "logSoftmax = nn.LogSoftmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: tensor(0.9953)\n",
      "Cost: tensor(0.9873)\n",
      "Cost: tensor(1.0196)\n",
      "Cost: tensor(0.9934)\n",
      "Cost: tensor(0.8609)\n",
      "Cost: tensor(0.7788)\n",
      "Cost: tensor(0.9846)\n",
      "Cost: tensor(0.9991)\n",
      "Cost: tensor(0.8916)\n",
      "Cost: tensor(0.9951)\n",
      "Cost: tensor(1.0513)\n",
      "Cost: tensor(0.9323)\n",
      "Cost: tensor(0.8984)\n",
      "Cost: tensor(1.0620)\n",
      "Cost: tensor(0.7973)\n",
      "Cost: tensor(0.8322)\n",
      "Cost: tensor(0.9807)\n",
      "Cost: tensor(0.8538)\n",
      "Cost: tensor(0.9035)\n",
      "Cost: tensor(0.7660)\n",
      "Cost: tensor(1.1165)\n",
      "Cost: tensor(1.0764)\n",
      "Cost: tensor(0.8922)\n",
      "Cost: tensor(1.0268)\n",
      "Cost: tensor(1.0143)\n",
      "Cost: tensor(1.0288)\n",
      "Cost: tensor(0.9959)\n",
      "Cost: tensor(0.7547)\n",
      "Cost: tensor(0.7721)\n",
      "Cost: tensor(0.8314)\n",
      "Cost: tensor(0.8550)\n",
      "Cost: tensor(1.1062)\n",
      "Cost: tensor(0.8996)\n",
      "Cost: tensor(0.8406)\n",
      "Cost: tensor(0.8364)\n",
      "Cost: tensor(0.8745)\n",
      "Cost: tensor(1.0797)\n",
      "Cost: tensor(1.0167)\n",
      "Cost: tensor(0.9199)\n",
      "Cost: tensor(0.6562)\n",
      "Cost: tensor(0.8150)\n",
      "Cost: tensor(0.8288)\n",
      "Cost: tensor(0.9312)\n",
      "Cost: tensor(1.0721)\n",
      "Cost: tensor(1.1956)\n",
      "Cost: tensor(0.7414)\n",
      "Cost: tensor(0.9456)\n",
      "Cost: tensor(0.9995)\n",
      "Cost: tensor(0.8721)\n",
      "Cost: tensor(1.1373)\n",
      "Cost: tensor(0.7617)\n",
      "Cost: tensor(1.0448)\n",
      "Cost: tensor(1.0241)\n",
      "Cost: tensor(1.0782)\n",
      "Cost: tensor(0.9351)\n",
      "Cost: tensor(0.7061)\n",
      "Cost: tensor(1.0092)\n",
      "Cost: tensor(0.8507)\n",
      "Cost: tensor(1.1463)\n",
      "Cost: tensor(0.9431)\n",
      "Cost: tensor(1.0399)\n",
      "Cost: tensor(0.8505)\n",
      "Cost: tensor(0.8220)\n",
      "Cost: tensor(0.9601)\n",
      "Cost: tensor(1.1112)\n",
      "Cost: tensor(0.9195)\n",
      "Cost: tensor(0.9748)\n",
      "Cost: tensor(1.1535)\n",
      "Cost: tensor(0.7294)\n",
      "Cost: tensor(0.7387)\n",
      "Cost: tensor(0.9439)\n",
      "Cost: tensor(0.9846)\n",
      "Cost: tensor(0.8039)\n",
      "Cost: tensor(0.9461)\n",
      "Cost: tensor(0.9893)\n",
      "Cost: tensor(1.0411)\n",
      "Cost: tensor(0.9132)\n",
      "Cost: tensor(0.8672)\n",
      "Cost: tensor(0.7488)\n",
      "Cost: tensor(1.0065)\n",
      "Cost: tensor(0.9124)\n",
      "Cost: tensor(1.0018)\n",
      "Cost: tensor(1.0606)\n",
      "Cost: tensor(1.0474)\n",
      "Cost: tensor(1.0213)\n",
      "Cost: tensor(0.9847)\n",
      "Cost: tensor(0.9076)\n",
      "Cost: tensor(0.8649)\n",
      "Cost: tensor(0.7747)\n",
      "Cost: tensor(0.9050)\n",
      "Cost: tensor(0.6903)\n",
      "Cost: tensor(0.8990)\n",
      "Cost: tensor(0.9162)\n",
      "Cost: tensor(0.8735)\n",
      "Cost: tensor(0.9661)\n",
      "Cost: tensor(0.8432)\n",
      "Cost: tensor(1.0226)\n",
      "Cost: tensor(0.8769)\n",
      "Cost: tensor(1.2005)\n",
      "Cost: tensor(0.8614)\n",
      "Cost: tensor(0.7285)\n",
      "Cost: tensor(0.7936)\n",
      "Cost: tensor(0.7022)\n",
      "Cost: tensor(1.0419)\n",
      "Cost: tensor(0.7407)\n",
      "Cost: tensor(0.8270)\n",
      "Cost: tensor(1.0193)\n",
      "Cost: tensor(0.8880)\n",
      "Cost: tensor(0.9042)\n",
      "Cost: tensor(0.8285)\n",
      "Cost: tensor(0.7937)\n",
      "Cost: tensor(0.9965)\n",
      "Cost: tensor(0.9325)\n",
      "Cost: tensor(0.7384)\n",
      "Cost: tensor(0.8835)\n",
      "Cost: tensor(0.7901)\n",
      "Cost: tensor(1.1364)\n",
      "Cost: tensor(1.1231)\n",
      "Cost: tensor(0.9544)\n",
      "Cost: tensor(0.4998)\n",
      "Cost: tensor(0.9058)\n",
      "Cost: tensor(0.9900)\n",
      "Cost: tensor(0.8879)\n",
      "Cost: tensor(0.7317)\n",
      "Cost: tensor(0.9034)\n",
      "Cost: tensor(0.9214)\n",
      "Cost: tensor(0.8082)\n",
      "Cost: tensor(1.1169)\n",
      "Cost: tensor(0.9110)\n",
      "Cost: tensor(0.7380)\n",
      "Cost: tensor(0.8121)\n",
      "Cost: tensor(0.7297)\n",
      "Cost: tensor(0.7745)\n",
      "Cost: tensor(0.8856)\n",
      "Cost: tensor(1.1242)\n",
      "Cost: tensor(1.1839)\n",
      "Cost: tensor(0.7381)\n",
      "Cost: tensor(1.0710)\n",
      "Cost: tensor(0.8157)\n",
      "Cost: tensor(0.8199)\n",
      "Cost: tensor(0.8853)\n",
      "Cost: tensor(0.8197)\n",
      "Cost: tensor(0.8868)\n",
      "Cost: tensor(1.2160)\n",
      "Cost: tensor(1.1054)\n",
      "Cost: tensor(0.9446)\n",
      "Cost: tensor(0.9543)\n",
      "Cost: tensor(0.9290)\n",
      "Cost: tensor(0.8825)\n",
      "Cost: tensor(0.9387)\n",
      "Evaluating\n",
      "Dev Accuracy: 0.6196013289036545\n",
      "Cost: tensor(0.9590)\n",
      "Cost: tensor(1.0728)\n",
      "Cost: tensor(0.7727)\n",
      "Cost: tensor(0.9429)\n",
      "Cost: tensor(1.0685)\n",
      "Cost: tensor(0.9426)\n",
      "Cost: tensor(0.9096)\n",
      "Cost: tensor(0.7571)\n",
      "Cost: tensor(0.7676)\n",
      "Cost: tensor(0.9395)\n",
      "Cost: tensor(0.8393)\n",
      "Cost: tensor(1.0085)\n",
      "Cost: tensor(0.8404)\n",
      "Cost: tensor(0.8380)\n",
      "Cost: tensor(0.7490)\n",
      "Cost: tensor(0.6778)\n",
      "Cost: tensor(1.0479)\n",
      "Cost: tensor(0.8419)\n",
      "Cost: tensor(0.8701)\n",
      "Cost: tensor(1.0223)\n",
      "Cost: tensor(0.9253)\n",
      "Cost: tensor(0.7805)\n",
      "Cost: tensor(0.8343)\n",
      "Cost: tensor(0.8418)\n",
      "Cost: tensor(0.8221)\n",
      "Cost: tensor(0.8823)\n",
      "Cost: tensor(0.8825)\n",
      "Cost: tensor(1.0395)\n",
      "Cost: tensor(0.8457)\n",
      "Cost: tensor(0.8455)\n",
      "Cost: tensor(0.7805)\n",
      "Cost: tensor(0.8584)\n",
      "Cost: tensor(0.7513)\n",
      "Cost: tensor(0.8495)\n",
      "Cost: tensor(1.0071)\n",
      "Cost: tensor(1.0795)\n",
      "Cost: tensor(1.0345)\n",
      "Cost: tensor(0.8778)\n",
      "Cost: tensor(0.8932)\n",
      "Cost: tensor(1.0153)\n",
      "Cost: tensor(0.9994)\n",
      "Cost: tensor(0.9783)\n",
      "Cost: tensor(1.0712)\n",
      "Cost: tensor(0.8443)\n",
      "Cost: tensor(0.7879)\n",
      "Cost: tensor(0.5844)\n",
      "Cost: tensor(0.6815)\n",
      "Cost: tensor(0.7291)\n",
      "Cost: tensor(1.0464)\n",
      "Cost: tensor(0.8240)\n",
      "Cost: tensor(1.0071)\n",
      "Cost: tensor(0.7556)\n",
      "Cost: tensor(1.0130)\n",
      "Cost: tensor(0.9473)\n",
      "Cost: tensor(0.9397)\n",
      "Cost: tensor(0.8288)\n",
      "Cost: tensor(0.8623)\n",
      "Cost: tensor(0.8354)\n",
      "Cost: tensor(0.6920)\n",
      "Cost: tensor(0.8814)\n",
      "Cost: tensor(0.8941)\n",
      "Cost: tensor(0.9136)\n",
      "Cost: tensor(0.9714)\n",
      "Cost: tensor(0.8650)\n",
      "Cost: tensor(0.9381)\n",
      "Cost: tensor(0.7443)\n",
      "Cost: tensor(0.9738)\n",
      "Cost: tensor(0.8040)\n",
      "Cost: tensor(0.8287)\n",
      "Cost: tensor(0.9671)\n",
      "Cost: tensor(0.8943)\n",
      "Cost: tensor(0.9213)\n",
      "Cost: tensor(1.0763)\n",
      "Cost: tensor(0.7269)\n",
      "Cost: tensor(0.9449)\n",
      "Cost: tensor(0.9593)\n",
      "Cost: tensor(0.7908)\n",
      "Cost: tensor(0.7561)\n",
      "Cost: tensor(0.9495)\n",
      "Cost: tensor(0.8582)\n",
      "Cost: tensor(1.0351)\n",
      "Cost: tensor(0.7596)\n",
      "Cost: tensor(0.9165)\n",
      "Cost: tensor(0.7198)\n",
      "Cost: tensor(0.9380)\n",
      "Cost: tensor(0.7469)\n",
      "Cost: tensor(0.8693)\n",
      "Cost: tensor(0.8081)\n",
      "Cost: tensor(1.0614)\n",
      "Cost: tensor(0.9978)\n",
      "Cost: tensor(0.9581)\n",
      "Cost: tensor(0.7819)\n",
      "Cost: tensor(0.9089)\n",
      "Cost: tensor(1.0189)\n",
      "Cost: tensor(0.9378)\n",
      "Cost: tensor(0.8499)\n",
      "Cost: tensor(1.0121)\n",
      "Cost: tensor(0.7275)\n",
      "Cost: tensor(1.0395)\n",
      "Cost: tensor(0.8527)\n",
      "Cost: tensor(0.9695)\n",
      "Cost: tensor(0.9609)\n",
      "Cost: tensor(0.9188)\n",
      "Cost: tensor(0.6754)\n",
      "Cost: tensor(0.7232)\n",
      "Cost: tensor(1.0313)\n",
      "Cost: tensor(0.8169)\n",
      "Cost: tensor(0.8809)\n",
      "Cost: tensor(0.7236)\n",
      "Cost: tensor(0.9657)\n",
      "Cost: tensor(0.6677)\n",
      "Cost: tensor(1.0438)\n",
      "Cost: tensor(0.8975)\n",
      "Cost: tensor(0.8866)\n",
      "Cost: tensor(0.9289)\n",
      "Cost: tensor(1.0650)\n",
      "Cost: tensor(1.0954)\n",
      "Cost: tensor(0.8284)\n",
      "Cost: tensor(0.7476)\n",
      "Cost: tensor(0.8569)\n",
      "Cost: tensor(0.8188)\n",
      "Cost: tensor(0.7639)\n",
      "Cost: tensor(0.9034)\n",
      "Cost: tensor(0.8208)\n",
      "Cost: tensor(0.6809)\n",
      "Cost: tensor(1.0034)\n",
      "Cost: tensor(0.8436)\n",
      "Cost: tensor(0.8704)\n",
      "Cost: tensor(0.7663)\n",
      "Cost: tensor(0.9991)\n",
      "Cost: tensor(1.0608)\n",
      "Cost: tensor(0.9065)\n",
      "Cost: tensor(0.6289)\n",
      "Cost: tensor(1.0753)\n",
      "Cost: tensor(0.8898)\n",
      "Cost: tensor(1.0242)\n",
      "Cost: tensor(0.9268)\n",
      "Cost: tensor(1.1682)\n",
      "Cost: tensor(0.8915)\n",
      "Cost: tensor(0.8181)\n",
      "Cost: tensor(0.7745)\n",
      "Cost: tensor(0.7571)\n",
      "Cost: tensor(0.9145)\n",
      "Cost: tensor(0.5712)\n",
      "Cost: tensor(1.0033)\n",
      "Cost: tensor(0.6650)\n",
      "Cost: tensor(0.8069)\n",
      "Cost: tensor(0.9758)\n",
      "Cost: tensor(0.8533)\n",
      "Cost: tensor(1.0658)\n",
      "Evaluating\n",
      "Dev Accuracy: 0.6212624584717608\n",
      "Cost: tensor(0.8021)\n",
      "Cost: tensor(0.5870)\n",
      "Cost: tensor(1.0389)\n",
      "Cost: tensor(0.8633)\n",
      "Cost: tensor(0.7202)\n",
      "Cost: tensor(0.5373)\n",
      "Cost: tensor(0.9043)\n",
      "Cost: tensor(0.9155)\n",
      "Cost: tensor(0.7956)\n",
      "Cost: tensor(0.7913)\n",
      "Cost: tensor(0.7755)\n",
      "Cost: tensor(0.7518)\n",
      "Cost: tensor(0.7074)\n",
      "Cost: tensor(0.8244)\n",
      "Cost: tensor(0.7414)\n",
      "Cost: tensor(0.9844)\n",
      "Cost: tensor(0.6008)\n",
      "Cost: tensor(0.9595)\n",
      "Cost: tensor(0.5986)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-54e8200d02df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cost:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 20\n",
    "loops = 150\n",
    "for _ in range(epochs):\n",
    "    for i in range(loops):\n",
    "        indices = np.random.choice(len(train_data), batch_size)\n",
    "        outputs = []\n",
    "        types = []\n",
    "        optimizer.zero_grad()\n",
    "        for j in indices:\n",
    "            tree, target, label = train_data[j]\n",
    "            output = model(tree, target)[-1]#3\n",
    "            output = logSoftmax(output)\n",
    "            outputs.append(output)\n",
    "            types.append(label)\n",
    "    \n",
    "        outputs = torch.stack(outputs)    \n",
    "        cost = loss(outputs, torch.LongTensor(types))\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        print('Cost:', cost)\n",
    "    print('Evaluating')\n",
    "    count = 0\n",
    "    for tree, target, label in dev_data:\n",
    "        output = model(tree, target)[-1]\n",
    "        pred = output.argmax().item()\n",
    "        if pred == label:\n",
    "            count += 1\n",
    "    print('Dev Accuracy:', count*1.0/len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0190, -1.6651, -1.0971])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
